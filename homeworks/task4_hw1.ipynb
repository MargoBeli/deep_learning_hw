{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-08T13:40:00.285614Z",
     "start_time": "2024-10-08T13:39:29.632783Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T14:47:12.908665Z",
     "start_time": "2024-10-08T14:47:12.292672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x = pd.read_csv('train_x.csv')\n",
    "train_y = pd.read_csv('train_y.csv')\n",
    "test_x = pd.read_csv('test_x.csv')\n",
    "\n",
    "X = train_x.iloc[:, 1:].values\n",
    "y = train_y.iloc[:, 1].values\n",
    "test_ids = test_x.iloc[:, -1]\n",
    "test = test_x.iloc[:, :-1].values\n",
    "X.shape, y.shape, test_ids.shape, test.shape"
   ],
   "id": "1de8e5a392fb0bbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14000, 90), (14000,), (6000,), (6000, 90))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T14:47:13.034995Z",
     "start_time": "2024-10-08T14:47:13.008437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ],
   "id": "b921f80b380ba345",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11200, 90), (2800, 90), (11200,), (2800,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Преобразование данных в тензоры",
   "id": "1081421ebc28083f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T14:47:13.226268Z",
     "start_time": "2024-10-08T14:47:13.200013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "id": "5a750b84e3cafc1f",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:09:51.064282Z",
     "start_time": "2024-10-08T15:09:51.054646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(90, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = torch.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ],
   "id": "b9505e66fea75c0d",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:25:03.419584Z",
     "start_time": "2024-10-08T15:25:03.407616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def early_stopping(val_loss, best_loss, patience_counter, patience=5, min_delta=0):\n",
    "    if best_loss is None:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    elif val_loss > best_loss - min_delta:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            return True, best_loss, patience_counter\n",
    "    else:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "    return False, best_loss, patience_counter"
   ],
   "id": "ed66f6694c0d9d3f",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:18:04.667641Z",
     "start_time": "2024-10-08T15:18:04.658259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=1000, patience=5, min_delta=0):\n",
    "    best_loss = None\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        should_stop, best_loss, patience_counter = early_stopping(val_loss, best_loss, patience_counter, patience, min_delta)\n",
    "        if should_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ],
   "id": "bcab63da27e363b0",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:25:03.104034Z",
     "start_time": "2024-10-08T15:18:04.823632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MyNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=1000, patience=100)"
   ],
   "id": "4d7bf5a41d497b11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 2889805.7207, Validation Loss: 847865.4759\n",
      "Epoch 2/1000, Train Loss: 302289.9946, Validation Loss: 27520.2061\n",
      "Epoch 3/1000, Train Loss: 5931.0154, Validation Loss: 2539.3799\n",
      "Epoch 4/1000, Train Loss: 2067.3631, Validation Loss: 933.5361\n",
      "Epoch 5/1000, Train Loss: 1810.0153, Validation Loss: 2672.4907\n",
      "Epoch 6/1000, Train Loss: 1445.8799, Validation Loss: 3766.6726\n",
      "Epoch 7/1000, Train Loss: 1288.8727, Validation Loss: 6661.4756\n",
      "Epoch 8/1000, Train Loss: 1116.3130, Validation Loss: 1039.6281\n",
      "Epoch 9/1000, Train Loss: 649.6243, Validation Loss: 1514.3322\n",
      "Epoch 10/1000, Train Loss: 733.3752, Validation Loss: 630.1478\n",
      "Epoch 11/1000, Train Loss: 759.1089, Validation Loss: 924.6052\n",
      "Epoch 12/1000, Train Loss: 551.8028, Validation Loss: 786.1324\n",
      "Epoch 13/1000, Train Loss: 500.1745, Validation Loss: 812.8589\n",
      "Epoch 14/1000, Train Loss: 450.5180, Validation Loss: 1758.8065\n",
      "Epoch 15/1000, Train Loss: 495.9458, Validation Loss: 1404.3554\n",
      "Epoch 16/1000, Train Loss: 809.7069, Validation Loss: 1160.1403\n",
      "Epoch 17/1000, Train Loss: 491.9674, Validation Loss: 469.4977\n",
      "Epoch 18/1000, Train Loss: 349.4023, Validation Loss: 2283.4834\n",
      "Epoch 19/1000, Train Loss: 363.3017, Validation Loss: 318.3380\n",
      "Epoch 20/1000, Train Loss: 520.4511, Validation Loss: 3200.3317\n",
      "Epoch 21/1000, Train Loss: 1420.7829, Validation Loss: 261.8457\n",
      "Epoch 22/1000, Train Loss: 493.5572, Validation Loss: 373.8297\n",
      "Epoch 23/1000, Train Loss: 378.0679, Validation Loss: 544.8592\n",
      "Epoch 24/1000, Train Loss: 307.6309, Validation Loss: 246.8073\n",
      "Epoch 25/1000, Train Loss: 327.0719, Validation Loss: 500.7083\n",
      "Epoch 26/1000, Train Loss: 369.5305, Validation Loss: 1285.0314\n",
      "Epoch 27/1000, Train Loss: 370.1000, Validation Loss: 333.6537\n",
      "Epoch 28/1000, Train Loss: 221.4658, Validation Loss: 313.3372\n",
      "Epoch 29/1000, Train Loss: 405.6536, Validation Loss: 728.7630\n",
      "Epoch 30/1000, Train Loss: 295.6596, Validation Loss: 654.5385\n",
      "Epoch 31/1000, Train Loss: 223.8241, Validation Loss: 219.7640\n",
      "Epoch 32/1000, Train Loss: 193.8966, Validation Loss: 328.3676\n",
      "Epoch 33/1000, Train Loss: 160.7608, Validation Loss: 779.7168\n",
      "Epoch 34/1000, Train Loss: 359.7127, Validation Loss: 222.2257\n",
      "Epoch 35/1000, Train Loss: 207.8559, Validation Loss: 615.8094\n",
      "Epoch 36/1000, Train Loss: 251.1773, Validation Loss: 1006.1529\n",
      "Epoch 37/1000, Train Loss: 230.1439, Validation Loss: 222.1540\n",
      "Epoch 38/1000, Train Loss: 173.3120, Validation Loss: 295.6829\n",
      "Epoch 39/1000, Train Loss: 163.4881, Validation Loss: 445.1293\n",
      "Epoch 40/1000, Train Loss: 160.2820, Validation Loss: 258.6803\n",
      "Epoch 41/1000, Train Loss: 153.9071, Validation Loss: 190.9322\n",
      "Epoch 42/1000, Train Loss: 157.3933, Validation Loss: 485.7293\n",
      "Epoch 43/1000, Train Loss: 144.1274, Validation Loss: 321.4466\n",
      "Epoch 44/1000, Train Loss: 153.6773, Validation Loss: 152.1451\n",
      "Epoch 45/1000, Train Loss: 144.2424, Validation Loss: 344.8806\n",
      "Epoch 46/1000, Train Loss: 175.5894, Validation Loss: 305.6776\n",
      "Epoch 47/1000, Train Loss: 137.2740, Validation Loss: 596.7872\n",
      "Epoch 48/1000, Train Loss: 139.0074, Validation Loss: 312.2071\n",
      "Epoch 49/1000, Train Loss: 135.3115, Validation Loss: 189.6312\n",
      "Epoch 50/1000, Train Loss: 138.2249, Validation Loss: 212.1534\n",
      "Epoch 51/1000, Train Loss: 133.5323, Validation Loss: 224.3009\n",
      "Epoch 52/1000, Train Loss: 123.8062, Validation Loss: 718.7933\n",
      "Epoch 53/1000, Train Loss: 148.4369, Validation Loss: 138.0271\n",
      "Epoch 54/1000, Train Loss: 129.3385, Validation Loss: 658.7052\n",
      "Epoch 55/1000, Train Loss: 119.0704, Validation Loss: 466.3023\n",
      "Epoch 56/1000, Train Loss: 117.3648, Validation Loss: 423.0900\n",
      "Epoch 57/1000, Train Loss: 113.7102, Validation Loss: 542.6372\n",
      "Epoch 58/1000, Train Loss: 131.1860, Validation Loss: 832.0818\n",
      "Epoch 59/1000, Train Loss: 127.1169, Validation Loss: 330.4202\n",
      "Epoch 60/1000, Train Loss: 119.3752, Validation Loss: 264.9938\n",
      "Epoch 61/1000, Train Loss: 121.7283, Validation Loss: 243.7924\n",
      "Epoch 62/1000, Train Loss: 113.7050, Validation Loss: 136.0248\n",
      "Epoch 63/1000, Train Loss: 112.2346, Validation Loss: 256.8014\n",
      "Epoch 64/1000, Train Loss: 114.7593, Validation Loss: 196.9492\n",
      "Epoch 65/1000, Train Loss: 110.8625, Validation Loss: 129.8381\n",
      "Epoch 66/1000, Train Loss: 105.2287, Validation Loss: 216.4489\n",
      "Epoch 67/1000, Train Loss: 113.7744, Validation Loss: 296.5655\n",
      "Epoch 68/1000, Train Loss: 106.5916, Validation Loss: 184.2539\n",
      "Epoch 69/1000, Train Loss: 114.2931, Validation Loss: 164.9332\n",
      "Epoch 70/1000, Train Loss: 103.4604, Validation Loss: 146.7964\n",
      "Epoch 71/1000, Train Loss: 107.0581, Validation Loss: 103.1521\n",
      "Epoch 72/1000, Train Loss: 122.3549, Validation Loss: 243.4270\n",
      "Epoch 73/1000, Train Loss: 106.3227, Validation Loss: 157.8932\n",
      "Epoch 74/1000, Train Loss: 110.1183, Validation Loss: 120.2737\n",
      "Epoch 75/1000, Train Loss: 101.8242, Validation Loss: 118.0893\n",
      "Epoch 76/1000, Train Loss: 104.8001, Validation Loss: 196.4496\n",
      "Epoch 77/1000, Train Loss: 105.9926, Validation Loss: 301.7166\n",
      "Epoch 78/1000, Train Loss: 101.3001, Validation Loss: 324.7528\n",
      "Epoch 79/1000, Train Loss: 109.3473, Validation Loss: 319.6672\n",
      "Epoch 80/1000, Train Loss: 103.5451, Validation Loss: 219.7378\n",
      "Epoch 81/1000, Train Loss: 100.1431, Validation Loss: 151.8855\n",
      "Epoch 82/1000, Train Loss: 103.4962, Validation Loss: 115.2789\n",
      "Epoch 83/1000, Train Loss: 94.7654, Validation Loss: 157.3694\n",
      "Epoch 84/1000, Train Loss: 94.6850, Validation Loss: 115.0460\n",
      "Epoch 85/1000, Train Loss: 94.4582, Validation Loss: 146.1503\n",
      "Epoch 86/1000, Train Loss: 92.7140, Validation Loss: 222.5750\n",
      "Epoch 87/1000, Train Loss: 93.4187, Validation Loss: 103.9348\n",
      "Epoch 88/1000, Train Loss: 92.7798, Validation Loss: 130.6952\n",
      "Epoch 89/1000, Train Loss: 92.7965, Validation Loss: 135.8637\n",
      "Epoch 90/1000, Train Loss: 90.6620, Validation Loss: 134.5506\n",
      "Epoch 91/1000, Train Loss: 92.0242, Validation Loss: 141.0969\n",
      "Epoch 92/1000, Train Loss: 91.3970, Validation Loss: 100.4622\n",
      "Epoch 93/1000, Train Loss: 92.0797, Validation Loss: 117.8014\n",
      "Epoch 94/1000, Train Loss: 91.1111, Validation Loss: 117.0638\n",
      "Epoch 95/1000, Train Loss: 91.3111, Validation Loss: 109.4730\n",
      "Epoch 96/1000, Train Loss: 94.1566, Validation Loss: 149.3088\n",
      "Epoch 97/1000, Train Loss: 93.8496, Validation Loss: 105.9922\n",
      "Epoch 98/1000, Train Loss: 91.8479, Validation Loss: 157.3950\n",
      "Epoch 99/1000, Train Loss: 91.7720, Validation Loss: 101.9203\n",
      "Epoch 100/1000, Train Loss: 92.1839, Validation Loss: 242.2084\n",
      "Epoch 101/1000, Train Loss: 91.4651, Validation Loss: 126.0681\n",
      "Epoch 102/1000, Train Loss: 91.6999, Validation Loss: 100.1793\n",
      "Epoch 103/1000, Train Loss: 91.1652, Validation Loss: 97.2955\n",
      "Epoch 104/1000, Train Loss: 90.9040, Validation Loss: 120.3956\n",
      "Epoch 105/1000, Train Loss: 91.0722, Validation Loss: 116.1640\n",
      "Epoch 106/1000, Train Loss: 90.8546, Validation Loss: 94.8308\n",
      "Epoch 107/1000, Train Loss: 91.4880, Validation Loss: 91.0070\n",
      "Epoch 108/1000, Train Loss: 90.4429, Validation Loss: 127.5423\n",
      "Epoch 109/1000, Train Loss: 92.3162, Validation Loss: 114.3043\n",
      "Epoch 110/1000, Train Loss: 89.1226, Validation Loss: 182.1261\n",
      "Epoch 111/1000, Train Loss: 91.8939, Validation Loss: 140.8568\n",
      "Epoch 112/1000, Train Loss: 90.4784, Validation Loss: 103.5377\n",
      "Epoch 113/1000, Train Loss: 91.6176, Validation Loss: 97.3294\n",
      "Epoch 114/1000, Train Loss: 91.8842, Validation Loss: 102.7607\n",
      "Epoch 115/1000, Train Loss: 92.9341, Validation Loss: 137.6923\n",
      "Epoch 116/1000, Train Loss: 89.5805, Validation Loss: 109.3565\n",
      "Epoch 117/1000, Train Loss: 90.6552, Validation Loss: 108.1453\n",
      "Epoch 118/1000, Train Loss: 91.3837, Validation Loss: 92.9718\n",
      "Epoch 119/1000, Train Loss: 85.8926, Validation Loss: 102.6981\n",
      "Epoch 120/1000, Train Loss: 84.4157, Validation Loss: 90.3380\n",
      "Epoch 121/1000, Train Loss: 85.0881, Validation Loss: 94.1824\n",
      "Epoch 122/1000, Train Loss: 85.7129, Validation Loss: 91.1368\n",
      "Epoch 123/1000, Train Loss: 83.9119, Validation Loss: 95.5978\n",
      "Epoch 124/1000, Train Loss: 84.6714, Validation Loss: 103.4779\n",
      "Epoch 125/1000, Train Loss: 83.9703, Validation Loss: 91.8322\n",
      "Epoch 126/1000, Train Loss: 84.4576, Validation Loss: 93.3053\n",
      "Epoch 127/1000, Train Loss: 84.1750, Validation Loss: 89.6027\n",
      "Epoch 128/1000, Train Loss: 83.5712, Validation Loss: 96.8311\n",
      "Epoch 129/1000, Train Loss: 83.5312, Validation Loss: 93.0078\n",
      "Epoch 130/1000, Train Loss: 85.1797, Validation Loss: 140.9132\n",
      "Epoch 131/1000, Train Loss: 85.1637, Validation Loss: 98.1047\n",
      "Epoch 132/1000, Train Loss: 85.0678, Validation Loss: 97.2814\n",
      "Epoch 133/1000, Train Loss: 85.8632, Validation Loss: 88.4105\n",
      "Epoch 134/1000, Train Loss: 84.1078, Validation Loss: 99.8011\n",
      "Epoch 135/1000, Train Loss: 83.5160, Validation Loss: 96.7627\n",
      "Epoch 136/1000, Train Loss: 84.1119, Validation Loss: 92.2874\n",
      "Epoch 137/1000, Train Loss: 84.0482, Validation Loss: 94.3355\n",
      "Epoch 138/1000, Train Loss: 83.2530, Validation Loss: 94.1519\n",
      "Epoch 139/1000, Train Loss: 84.3666, Validation Loss: 110.0658\n",
      "Epoch 140/1000, Train Loss: 84.3245, Validation Loss: 92.5260\n",
      "Epoch 141/1000, Train Loss: 83.3327, Validation Loss: 98.8227\n",
      "Epoch 142/1000, Train Loss: 83.5741, Validation Loss: 116.8392\n",
      "Epoch 143/1000, Train Loss: 84.0629, Validation Loss: 91.7046\n",
      "Epoch 144/1000, Train Loss: 84.4280, Validation Loss: 93.4485\n",
      "Epoch 145/1000, Train Loss: 80.0763, Validation Loss: 90.5388\n",
      "Epoch 146/1000, Train Loss: 80.0734, Validation Loss: 90.6242\n",
      "Epoch 147/1000, Train Loss: 80.2114, Validation Loss: 91.3192\n",
      "Epoch 148/1000, Train Loss: 80.7998, Validation Loss: 91.4171\n",
      "Epoch 149/1000, Train Loss: 80.4899, Validation Loss: 90.2460\n",
      "Epoch 150/1000, Train Loss: 79.3286, Validation Loss: 89.3762\n",
      "Epoch 151/1000, Train Loss: 80.2851, Validation Loss: 91.3843\n",
      "Epoch 152/1000, Train Loss: 80.0677, Validation Loss: 94.6463\n",
      "Epoch 153/1000, Train Loss: 80.1223, Validation Loss: 102.9202\n",
      "Epoch 154/1000, Train Loss: 80.3221, Validation Loss: 91.7310\n",
      "Epoch 155/1000, Train Loss: 79.3125, Validation Loss: 89.0025\n",
      "Epoch 156/1000, Train Loss: 79.2053, Validation Loss: 91.3467\n",
      "Epoch 157/1000, Train Loss: 78.0598, Validation Loss: 89.3061\n",
      "Epoch 158/1000, Train Loss: 78.0874, Validation Loss: 89.1232\n",
      "Epoch 159/1000, Train Loss: 78.3982, Validation Loss: 90.3843\n",
      "Epoch 160/1000, Train Loss: 78.8688, Validation Loss: 90.5678\n",
      "Epoch 161/1000, Train Loss: 78.1328, Validation Loss: 91.6614\n",
      "Epoch 162/1000, Train Loss: 78.1221, Validation Loss: 92.3832\n",
      "Epoch 163/1000, Train Loss: 77.3614, Validation Loss: 91.5727\n",
      "Epoch 164/1000, Train Loss: 79.0289, Validation Loss: 93.9012\n",
      "Epoch 165/1000, Train Loss: 78.4552, Validation Loss: 91.4685\n",
      "Epoch 166/1000, Train Loss: 78.6148, Validation Loss: 90.9440\n",
      "Epoch 167/1000, Train Loss: 77.5072, Validation Loss: 90.7451\n",
      "Epoch 168/1000, Train Loss: 77.8254, Validation Loss: 90.4620\n",
      "Epoch 169/1000, Train Loss: 77.3561, Validation Loss: 89.2311\n",
      "Epoch 170/1000, Train Loss: 77.4222, Validation Loss: 90.7972\n",
      "Epoch 171/1000, Train Loss: 78.4046, Validation Loss: 90.4631\n",
      "Epoch 172/1000, Train Loss: 77.9821, Validation Loss: 90.4738\n",
      "Epoch 173/1000, Train Loss: 76.3298, Validation Loss: 98.1410\n",
      "Epoch 174/1000, Train Loss: 77.3039, Validation Loss: 90.6807\n",
      "Epoch 175/1000, Train Loss: 76.8881, Validation Loss: 94.0041\n",
      "Epoch 176/1000, Train Loss: 76.6500, Validation Loss: 89.3697\n",
      "Epoch 177/1000, Train Loss: 76.5230, Validation Loss: 89.1375\n",
      "Epoch 178/1000, Train Loss: 76.1053, Validation Loss: 90.5605\n",
      "Epoch 179/1000, Train Loss: 75.3399, Validation Loss: 92.7779\n",
      "Epoch 180/1000, Train Loss: 76.5426, Validation Loss: 95.0000\n",
      "Epoch 181/1000, Train Loss: 76.7655, Validation Loss: 96.6842\n",
      "Epoch 182/1000, Train Loss: 77.2256, Validation Loss: 90.2338\n",
      "Epoch 183/1000, Train Loss: 76.6601, Validation Loss: 89.4692\n",
      "Epoch 184/1000, Train Loss: 76.4635, Validation Loss: 91.8380\n",
      "Epoch 185/1000, Train Loss: 76.0826, Validation Loss: 92.9683\n",
      "Epoch 186/1000, Train Loss: 75.7134, Validation Loss: 92.5430\n",
      "Epoch 187/1000, Train Loss: 76.3134, Validation Loss: 94.0095\n",
      "Epoch 188/1000, Train Loss: 76.3291, Validation Loss: 90.6029\n",
      "Epoch 189/1000, Train Loss: 76.4807, Validation Loss: 89.6151\n",
      "Epoch 190/1000, Train Loss: 76.0128, Validation Loss: 90.1425\n",
      "Epoch 191/1000, Train Loss: 76.2778, Validation Loss: 91.8996\n",
      "Epoch 192/1000, Train Loss: 77.0083, Validation Loss: 90.0552\n",
      "Epoch 193/1000, Train Loss: 76.0837, Validation Loss: 89.7861\n",
      "Epoch 194/1000, Train Loss: 76.9814, Validation Loss: 94.0946\n",
      "Epoch 195/1000, Train Loss: 76.0123, Validation Loss: 89.6627\n",
      "Epoch 196/1000, Train Loss: 76.4416, Validation Loss: 96.7034\n",
      "Epoch 197/1000, Train Loss: 76.5903, Validation Loss: 93.9907\n",
      "Epoch 198/1000, Train Loss: 76.3586, Validation Loss: 91.6508\n",
      "Epoch 199/1000, Train Loss: 75.5847, Validation Loss: 95.1606\n",
      "Epoch 200/1000, Train Loss: 76.5242, Validation Loss: 91.5832\n",
      "Epoch 201/1000, Train Loss: 76.4668, Validation Loss: 113.1312\n",
      "Epoch 202/1000, Train Loss: 75.8961, Validation Loss: 93.4075\n",
      "Epoch 203/1000, Train Loss: 76.3946, Validation Loss: 90.5790\n",
      "Epoch 204/1000, Train Loss: 75.5842, Validation Loss: 93.0146\n",
      "Epoch 205/1000, Train Loss: 76.5302, Validation Loss: 93.7070\n",
      "Epoch 206/1000, Train Loss: 76.7702, Validation Loss: 90.6124\n",
      "Epoch 207/1000, Train Loss: 76.8586, Validation Loss: 90.3901\n",
      "Epoch 208/1000, Train Loss: 76.4888, Validation Loss: 89.2929\n",
      "Epoch 209/1000, Train Loss: 75.4546, Validation Loss: 91.7232\n",
      "Epoch 210/1000, Train Loss: 76.5661, Validation Loss: 91.4369\n",
      "Epoch 211/1000, Train Loss: 75.4823, Validation Loss: 99.9322\n",
      "Epoch 212/1000, Train Loss: 76.6648, Validation Loss: 95.5167\n",
      "Epoch 213/1000, Train Loss: 75.5142, Validation Loss: 104.6581\n",
      "Epoch 214/1000, Train Loss: 76.0425, Validation Loss: 91.8490\n",
      "Epoch 215/1000, Train Loss: 76.1373, Validation Loss: 91.0196\n",
      "Epoch 216/1000, Train Loss: 74.9464, Validation Loss: 99.6730\n",
      "Epoch 217/1000, Train Loss: 75.5947, Validation Loss: 90.5655\n",
      "Epoch 218/1000, Train Loss: 76.3203, Validation Loss: 89.4454\n",
      "Epoch 219/1000, Train Loss: 74.7532, Validation Loss: 91.8011\n",
      "Epoch 220/1000, Train Loss: 75.6698, Validation Loss: 91.1788\n",
      "Epoch 221/1000, Train Loss: 74.7542, Validation Loss: 91.5242\n",
      "Epoch 222/1000, Train Loss: 75.8247, Validation Loss: 89.1561\n",
      "Epoch 223/1000, Train Loss: 76.8268, Validation Loss: 92.0390\n",
      "Epoch 224/1000, Train Loss: 76.3499, Validation Loss: 91.3368\n",
      "Epoch 225/1000, Train Loss: 74.9827, Validation Loss: 90.8127\n",
      "Epoch 226/1000, Train Loss: 76.7198, Validation Loss: 92.7541\n",
      "Epoch 227/1000, Train Loss: 76.7790, Validation Loss: 89.9422\n",
      "Epoch 228/1000, Train Loss: 75.4865, Validation Loss: 99.8022\n",
      "Epoch 229/1000, Train Loss: 74.2210, Validation Loss: 91.8769\n",
      "Epoch 230/1000, Train Loss: 75.7085, Validation Loss: 94.8487\n",
      "Epoch 231/1000, Train Loss: 75.1096, Validation Loss: 92.4406\n",
      "Epoch 232/1000, Train Loss: 75.6520, Validation Loss: 97.9154\n",
      "Epoch 233/1000, Train Loss: 76.1605, Validation Loss: 92.1948\n",
      "Early stopping at epoch 233\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:25:03.193336Z",
     "start_time": "2024-10-08T15:25:03.129964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "\n",
    "predicted_years = torch.round(predictions).numpy()\n",
    "output_df = pd.DataFrame({'index': test_ids, 'year': predicted_years})\n",
    "output_df.to_csv('submission.csv', index=False)"
   ],
   "id": "1ba94281255fe892",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2dfe00bab7f7589c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

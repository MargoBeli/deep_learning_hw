{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyO/P3ixwHtXqxqiW2OxYq+A",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Задание 1.\n",
    "( 6 баллов) Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков. Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-r7SPjmNotd3",
    "ExecuteTime": {
     "end_time": "2024-09-27T21:56:27.540625Z",
     "start_time": "2024-09-27T21:56:27.524908Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x, epsilon=0):\n",
    "    return torch.clamp((1 / (1 + torch.exp(-x))), min=epsilon, max=1-epsilon)\n",
    "\n",
    "\n",
    "def nll_loss(y_pred, y_true):\n",
    "    return -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred)).mean()\n",
    "\n",
    "def gradient_descent(weights, bias, dw, db, learning_rate=0.1):\n",
    "    with torch.no_grad():\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "    return weights, bias\n",
    "    \n",
    "def train_neuron(X, y, initial_weights, initial_bias, learning_rate=0.01, epochs=10, loss_fn=nll_loss, optimizer_fn=gradient_descent, epsilon=0):\n",
    "    loss_values = []\n",
    "    weights = initial_weights.clone().detach().requires_grad_(True)\n",
    "    bias = initial_bias.clone().detach().requires_grad_(True)\n",
    "    m_weights = torch.zeros_like(weights)\n",
    "    m_bias = torch.zeros_like(bias)\n",
    "    v_weights = torch.zeros_like(weights)\n",
    "    v_bias = torch.zeros_like(bias)\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z = torch.matmul(X, weights) + bias\n",
    "        y_pred = sigmoid(z, epsilon)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss_values.append(round(loss.item(), 4))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # weights, bias = gradient_descent(weights, bias, torch.matmul(X.t(), (y_pred - y)) / len(X), (y_pred - y).mean(), learning_rate)\n",
    "        weights, bias, m_weights, m_bias, v_weights, v_bias = optimizer_fn(weights, bias, m_weights, m_bias, v_weights, v_bias, epoch+1, learning_rate)\n",
    "\n",
    "        # Zero the gradients\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "\n",
    "    return weights, bias, loss_values"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:56:28.275737Z",
     "start_time": "2024-09-27T21:56:27.875073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('train_x.csv')\n",
    "y = pd.read_csv('train_y.csv')\n",
    "X = X.drop(X.columns[0], axis=1)\n",
    "y = y.drop(y.columns[0], axis=1)\n",
    "\n",
    "years_to_use = [2006, 2007]\n",
    "mask = y['year'].isin(years_to_use)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "year_mapping = {2006: 0, 2007: 1}\n",
    "y['year'] = y['year'].map(year_mapping)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y.values, dtype=torch.float32).squeeze()\n",
    "\n",
    "initial_weights = torch.zeros(X.shape[1], requires_grad=True)\n",
    "initial_bias = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "eps = 1e-5"
   ],
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:27:43.463943Z",
     "start_time": "2024-09-27T21:27:43.436309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "updated_weights, updated_bias, nll_values = train_neuron(X, y, initial_weights, initial_bias, learning_rate, epochs, epsilon=eps)\n",
    "\n",
    "print(\"Updated weights:\", updated_weights)\n",
    "print(\"Updated bias:\", updated_bias)\n",
    "print(\"NLL values:\", nll_values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: tensor([-2.7537e-02, -3.0597e-03,  1.8042e-02,  6.1462e-03, -6.4874e-02,\n",
      "         2.0049e-02,  2.6663e-02, -3.1438e-03,  3.4858e-03,  7.1858e-04,\n",
      "        -4.6254e-02, -2.0814e-02,  1.1613e-02,  3.0140e-02,  3.3262e-03,\n",
      "         1.1781e-02,  3.2995e-03,  8.3393e-03,  2.8283e-02, -3.4848e-02,\n",
      "         1.1471e-02,  4.5568e-04,  3.2473e-03, -1.0949e-02, -1.8882e-02,\n",
      "         1.9587e-02, -2.2203e-02,  1.7279e-02, -4.9612e-03, -3.6055e-02,\n",
      "         2.4571e-02, -1.2637e-02,  1.4778e-02, -5.0098e-04, -4.8418e-03,\n",
      "         1.6184e-02,  2.8679e-02,  2.4067e-02,  3.2423e-02,  4.6086e-02,\n",
      "        -2.4088e-02,  2.7174e-02, -1.2515e-02,  3.9274e-02, -1.4684e-02,\n",
      "        -3.0494e-02,  2.5081e-03, -1.4940e-02,  2.8401e-02, -7.4540e-03,\n",
      "         2.3903e-02,  6.1825e-04,  2.5888e-02,  2.2094e-02,  1.7626e-02,\n",
      "         7.9589e-03,  1.9820e-02, -5.8408e-03, -4.9160e-02,  4.2695e-02,\n",
      "        -2.0137e-02, -1.7455e-03, -3.8323e-02,  2.3120e-02,  1.5536e-02,\n",
      "        -1.3640e-02, -2.7970e-02,  1.5689e-02,  5.6571e-03,  1.9333e-02,\n",
      "        -1.4438e-02,  1.5031e-02,  5.8868e-02,  4.5009e-02,  5.0651e-03,\n",
      "        -8.8129e-03, -1.4914e-03, -3.8858e-02, -1.0272e-03,  4.1927e-02,\n",
      "         1.4361e-02, -2.7023e-03,  4.4269e-02, -7.4681e-03, -1.6179e-02,\n",
      "         3.5211e-02,  2.0524e-02,  5.0498e-03,  7.8583e-05,  1.5439e-02],\n",
      "       requires_grad=True)\n",
      "Updated bias: tensor(0.0496, requires_grad=True)\n",
      "NLL values: [0.6931, 0.6896, 0.6875, 0.6859, 0.6847, 0.6838, 0.683, 0.6823, 0.6818, 0.6813]\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Задание 2. {*}\n",
    "(10 баллов) Реализуйте базовые функции autograd. Можете вдохновиться видео от Andrej Karpathy. Напишите класс, аналогичный предоставленному классу 'Element', который реализует основные операции autograd: сложение, умножение и активацию ReLU. Класс должен обрабатывать скалярные объекты и правильно вычислять градиенты для этих операций посредством автоматического дифференцирования. Плюсом будет набор предоставленных тестов, оценивающих правильность вычислений. Большим плюсом будет, если тесты будут написаны с помощью unittest. Можно использовать только чистый torch (без использования autograd и torch.nn). За каждую нереализованную операцию будет вычитаться 3 балла. Пример: a = Element(2) b = Element(-3) c = Element(10) d = a + b * c e = d.relu() e.backward() print(a, b, c, d, e) Output: Element(data=2, grad=0) Element(data=-3, grad=10) Element(data=10, grad=-3) Element(data=-28, grad=1) Element(data=0, grad=1)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:33:56.722062Z",
     "start_time": "2024-10-08T15:33:56.708376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Element:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0  # Градиент элемента (изначально равен 0)\n",
    "        self._backward = lambda: None  # Функция для вычисления градиента\n",
    "        self._prev = set(_children)  # Множество предыдущих элементов\n",
    "        self._op = _op  # Операция, которая была применена для получения текущего элемента\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Возвращает строковое представление элемента, включающее его данные и градиент\n",
    "        return f\"Element(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Определяет операцию сложения для элементов\n",
    "        if not isinstance(other, Element):  # Если второе слагаемое другого типа, операция не определена\n",
    "            return NotImplemented\n",
    "\n",
    "        def _backward():\n",
    "            # Функция для вычисления градиента при сложении\n",
    "            self.grad += 1 * other.grad\n",
    "            other.grad += 1 * self.grad\n",
    "\n",
    "        # Создаем новый элемент, который является суммой двух элементов\n",
    "        result = Element(self.data + other.data, (self, other), _op='+')\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Определяет операцию умножения для элементов\n",
    "        if not isinstance(other, Element):  # Если второй множитель другого типа, операция не определена\n",
    "            return NotImplemented\n",
    "\n",
    "        def _backward():\n",
    "            # Функция для вычисления градиента при умножении\n",
    "            self.grad += other.data * self.grad\n",
    "            other.grad += self.data * self.grad\n",
    "\n",
    "        # Создаем новый элемент, который является произведением двух элементов\n",
    "        result = Element(self.data * other.data, (self, other), _op='*')\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def relu(self):\n",
    "        # Применяет функцию ReLU к элементу\n",
    "        def _backward():\n",
    "            # Функция для вычисления градиента при применении ReLU\n",
    "            self.grad += 1 * (self.data > 0) * self.grad\n",
    "\n",
    "        # Создаем новый элемент, который является результатом применения ReLU\n",
    "        result = Element(max(0, self.data), (self,), _op='relu')\n",
    "        result._backward = _backward\n",
    "        return result\n",
    "\n",
    "    def backward(self):\n",
    "        # Метод для вычисления градиентов\n",
    "        self.grad += 1  # Инициализируем градиент текущего элемента как 1\n",
    "        visited = set()  # Множество посещенных элементов\n",
    "        nodes = [self]  # Список элементов для обработки (первый элемент - текущий)\n",
    "\n",
    "        while nodes:\n",
    "            node = nodes.pop()  # Извлекаем элемент из списка\n",
    "            if node not in visited:\n",
    "                visited.add(node)  # Добавляем элемент в множество посещенных\n",
    "                node._backward()  # Вычисляем градиент для текущего элемента\n",
    "                nodes.extend(node._prev)  # Добавляем предыдущие элементы в список для обработки\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T15:33:56.740386Z",
     "start_time": "2024-10-08T15:33:56.728042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = Element(2)\n",
    "b = Element(-3)\n",
    "c = Element(10)\n",
    "\n",
    "d = a + b * c\n",
    "e = d.relu()\n",
    "e.backward()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element(data=2, grad=0)\n",
      "Element(data=-3, grad=0)\n",
      "Element(data=10, grad=0)\n",
      "Element(data=-28, grad=0)\n",
      "Element(data=0, grad=1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Задание 3.\n",
    "Реализуйте один из оптимизаторов на выбор. Придумайте и напишите тесты для проверки выбранного оптимизатора. Проведите обучение нейрона из первого задания с использованием оптимизатора, а не ванильного градиентного спуска. Также опишите идею алгоритма (+1 балл). {*} Можете реализовать более 1 алгоритма. Каждый следующий даст 1 балл."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:57:37.741782Z",
     "start_time": "2024-09-27T21:57:37.641684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adam(weights, bias, m_weights, m_bias, v_weights, v_bias, t, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "    weights_grad = weights.grad\n",
    "    bias_grad = bias.grad\n",
    "        \n",
    "    m_weights = beta1 * m_weights + (1 - beta1) * weights_grad\n",
    "    m_bias = beta1 * m_bias + (1 - beta1) * bias_grad\n",
    "    v_weights = beta2 * v_weights + (1 - beta2) * weights_grad**2\n",
    "    v_bias = beta2 * v_bias + (1 - beta2) * bias_grad**2\n",
    "    \n",
    "    m_hat_weights = m_weights / (1 - beta1**t)\n",
    "    m_hat_bias = m_bias / (1 - beta1**t)\n",
    "    v_hat_weights = v_weights / (1 - beta2**t)\n",
    "    v_hat_bias = v_bias / (1 - beta2**t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # weight decay\n",
    "        weights *= (1 - lr * weight_decay)\n",
    "        bias *= (1 - lr * weight_decay)\n",
    "        \n",
    "        weights -= lr * m_hat_weights / (torch.sqrt(v_hat_weights) + eps)\n",
    "        bias -= lr * m_hat_bias / (torch.sqrt(v_hat_bias) + eps)\n",
    "    return weights, bias, m_weights, m_bias, v_weights, v_bias\n",
    "\n",
    "initial_weights = torch.zeros(X.shape[1], requires_grad=True)\n",
    "initial_bias = torch.tensor(0.0, requires_grad=True)\n",
    "updated_weights, updated_bias, nll_values = train_neuron(X, y, initial_weights, initial_bias, learning_rate=0.05, epochs=50, optimizer_fn=adam, epsilon=eps)\n",
    "print(\"Updated weights:\", updated_weights)\n",
    "print(\"Updated bias:\", updated_bias)\n",
    "print(\"NLL values:\", nll_values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: tensor([-9.7896e-02,  1.3599e-02,  2.4764e-02,  5.7589e-02, -1.0610e-01,\n",
      "        -2.2773e-02,  2.2934e-02, -1.0068e-02,  2.4557e-02, -7.3225e-02,\n",
      "        -1.2689e-01, -3.0651e-02, -5.9331e-03,  9.5473e-02, -5.5735e-02,\n",
      "         3.5051e-02, -7.6120e-03,  6.7291e-02,  8.5745e-02, -2.3933e-01,\n",
      "         6.4303e-02,  7.6357e-02,  1.3402e-02, -4.5313e-02, -5.2229e-02,\n",
      "         3.7564e-02, -1.2236e-01,  4.5275e-02, -1.9747e-02, -1.0032e-01,\n",
      "         8.3689e-03, -9.3994e-03,  2.4281e-03,  2.0051e-04, -2.1780e-02,\n",
      "         2.3621e-02,  6.8958e-02,  3.1565e-02,  9.1768e-02,  8.5176e-02,\n",
      "        -6.0717e-02,  1.3983e-01, -6.0379e-02,  8.7173e-02, -4.7965e-02,\n",
      "        -2.9013e-02, -1.7942e-02,  4.9713e-02,  8.1554e-02, -4.4766e-02,\n",
      "         8.1845e-02, -2.4049e-02,  5.7293e-02,  1.9475e-02,  3.6777e-02,\n",
      "         2.4629e-02,  6.5339e-02, -1.2626e-02, -9.5825e-02,  2.0477e-01,\n",
      "        -1.2376e-02,  1.9611e-02, -1.0352e-01,  4.1156e-02,  1.0382e-01,\n",
      "        -5.3978e-02, -2.4556e-02,  1.6199e-02,  1.3985e-02,  1.1144e-03,\n",
      "        -1.2155e-02,  4.9682e-02,  1.0523e-01,  3.1463e-02,  2.2578e-02,\n",
      "         1.2666e-02,  1.6384e-02, -1.0809e-01,  8.3491e-02,  7.5063e-02,\n",
      "         2.1730e-02, -7.2091e-03,  1.2312e-01, -2.7695e-02, -3.0563e-02,\n",
      "         7.0366e-02, -1.1833e-03, -5.0570e-02, -6.6346e-04,  3.2164e-02],\n",
      "       requires_grad=True)\n",
      "Updated bias: tensor(0.0707, requires_grad=True)\n",
      "NLL values: [0.6931, 0.7252, 0.6959, 0.7068, 0.6906, 0.6849, 0.6912, 0.6903, 0.6843, 0.6828, 0.6845, 0.6836, 0.6811, 0.6807, 0.6814, 0.6806, 0.6789, 0.6785, 0.6793, 0.6792, 0.678, 0.6775, 0.6778, 0.6779, 0.6775, 0.6772, 0.6771, 0.6771, 0.6769, 0.6767, 0.6765, 0.6765, 0.6767, 0.6765, 0.6761, 0.6761, 0.6764, 0.6762, 0.676, 0.676, 0.6761, 0.676, 0.6758, 0.6759, 0.6759, 0.6758, 0.6758, 0.6759, 0.6758, 0.6758]\n"
     ]
    }
   ],
   "execution_count": 132
  }
 ]
}
